{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hands-on Exercise (4)\n",
    "\n",
    "**Quantum Machine Learning Applications to High-Energy Physics**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Event Classfication using Quantum Neural Networks\n",
    "\n",
    "First, we consider the problem of classifying signal events from background events using kinematic features in the events, one of the most representative tasks for machine learning in HEP data analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparation of training data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'QuantumCircuit' from 'qiskit' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m accuracy_score\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OrderedDict\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mqiskit\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m QuantumCircuit, transpile\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mqiskit\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcircuit\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Parameter, ParameterVector\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mqiskit\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcircuit\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlibrary\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TwoLocal, ZFeatureMap, ZZFeatureMap\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'QuantumCircuit' from 'qiskit' (unknown location)"
     ]
    }
   ],
   "source": [
    "%pip install qiskit-algorithms qiskit-optimization qiskit-machine-learning\n",
    "\n",
    "# Tested with python 3.10.11, qiskit 0.42.1, numpy 1.23.5, scipy 1.9.3\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import OrderedDict\n",
    "\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit.circuit import Parameter, ParameterVector\n",
    "from qiskit.circuit.library import TwoLocal, ZFeatureMap, ZZFeatureMap\n",
    "from qiskit.primitives import Estimator, Sampler, BackendEstimator\n",
    "from qiskit.quantum_info import SparsePauliOp, Statevector\n",
    "from qiskit_algorithms.gradients import ParamShiftEstimatorGradient\n",
    "from qiskit_algorithms.minimum_eigensolvers import VQE, NumPyMinimumEigensolver\n",
    "from qiskit_algorithms.optimizers import SPSA, COBYLA\n",
    "from qiskit_optimization.applications import OptimizationApplication\n",
    "from qiskit_ibm_runtime import Session, Sampler as RuntimeSampler\n",
    "from qiskit_ibm_runtime.accounts import AccountNotFoundError\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit_machine_learning.algorithms.classifiers import VQC\n",
    "from qiskit_machine_learning.kernels import FidelityQuantumKernel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read data from file\n",
    "df = pd.read_csv(\"data/SUSY_1K.csv\",\n",
    "                 names=('isSignal','lep1_pt','lep1_eta','lep1_phi','lep2_pt','lep2_eta',\n",
    "                        'lep2_phi','miss_ene','miss_phi','MET_rel','axial_MET','M_R','M_TR_2',\n",
    "                        'R','MT2','S_R','M_Delta_R','dPhi_r_b','cos_theta_r1'))\n",
    "\n",
    "# Number of input features for training\n",
    "feature_dim = 3\n",
    "\n",
    "# Feature variables\n",
    "if feature_dim == 3:\n",
    "    selected_features = ['lep1_pt', 'lep2_pt', 'miss_ene']\n",
    "elif feature_dim == 5:\n",
    "    selected_features = ['lep1_pt','lep2_pt','miss_ene','M_TR_2','M_Delta_R']\n",
    "elif feature_dim == 7:\n",
    "    selected_features = ['lep1_pt','lep1_eta','lep2_pt','lep2_eta','miss_ene','M_TR_2','M_Delta_R']\n",
    "\n",
    "# Number of events used in the training and testing\n",
    "train_size = 20\n",
    "test_size = 20\n",
    "\n",
    "df_sig = df.loc[df.isSignal==1, selected_features]\n",
    "df_bkg = df.loc[df.isSignal==0, selected_features]\n",
    "\n",
    "# Extract the samples\n",
    "df_sig_train = df_sig.values[:train_size]\n",
    "df_bkg_train = df_bkg.values[:train_size]\n",
    "df_sig_test = df_sig.values[train_size:train_size + test_size]\n",
    "df_bkg_test = df_bkg.values[train_size:train_size + test_size]\n",
    "# The first train_size events contain SUSY signal and the last train_size events do not.\n",
    "train_data = np.concatenate([df_sig_train, df_bkg_train])\n",
    "# The first test_size events contain SUSY signal and the last test_size events do not.\n",
    "test_data = np.concatenate([df_sig_test, df_bkg_test])\n",
    "\n",
    "# Label\n",
    "train_label = np.zeros(train_size * 2, dtype=int)\n",
    "train_label[:train_size] = 1\n",
    "test_label = np.zeros(train_size * 2, dtype=int)\n",
    "test_label[:test_size] = 1\n",
    "\n",
    "train_label_one_hot = np.zeros((train_size * 2, 2))\n",
    "train_label_one_hot[:train_size, 0] = 1\n",
    "train_label_one_hot[train_size:, 1] = 1\n",
    "test_label_one_hot = np.zeros((test_size * 2, 2))\n",
    "test_label_one_hot[:test_size, 0] = 1\n",
    "test_label_one_hot[test_size:, 1] = 1\n",
    "\n",
    "#datapoints, class_to_label = split_dataset_to_data_and_labels(test_input)\n",
    "#datapoints_tr, class_to_label_tr = split_dataset_to_data_and_labels(training_input)\n",
    "\n",
    "mms = MinMaxScaler((-1, 1))\n",
    "norm_train_data = mms.fit_transform(train_data)\n",
    "norm_test_data = mms.transform(test_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### State Preparation with Feature Map"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#feature_map = ZFeatureMap(feature_dimension=feature_dim, reps=1)\n",
    "feature_map = ZZFeatureMap(feature_dimension=feature_dim, reps=1, entanglement='circular')\n",
    "feature_map.decompose().draw('mpl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### State Transformation with Variational Form"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ansatz = TwoLocal(num_qubits=feature_dim, rotation_blocks=['ry', 'rz'], entanglement_blocks='cz', entanglement='circular', reps=3)\n",
    "#ansatz = TwoLocal(num_qubits=feature_dim, rotation_blocks=['ry'], entanglement_blocks='cz', entanglement='circular', reps=3)\n",
    "ansatz.decompose().draw('mpl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Measurement and Model Output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use Sampler instead of backend\n",
    "sampler = Sampler()\n",
    "\n",
    "# When using quantum hardware\n",
    "# instance = 'ibm-q/open/main'\n",
    "\n",
    "# try:\n",
    "#     service = QiskitRuntimeService(channel='ibm_quantum', instance=instance)\n",
    "# except AccountNotFoundError:\n",
    "#     service = QiskitRuntimeService(channel='ibm_quantum', token='__paste_your_token_here__',\n",
    "#                                    instance=instance)\n",
    "\n",
    "# backend_name = 'ibm_washington'\n",
    "# session = Session(service=service, backend=backend_name)\n",
    "\n",
    "# sampler = RuntimeSampler(session=session)\n",
    "\n",
    "maxiter = 300\n",
    "\n",
    "optimizer = COBYLA(maxiter=maxiter, disp=True)\n",
    "\n",
    "objective_func_vals = []\n",
    "# Draw the value of objective function every time when the fit() method is called\n",
    "def callback_graph(weights, obj_func_eval):\n",
    "    clear_output(wait=True)\n",
    "    objective_func_vals.append(obj_func_eval)\n",
    "    #print('obj_func_eval =',obj_func_eval)\n",
    "\n",
    "    plt.title(\"Objective function value against iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Objective function value\")\n",
    "    plt.plot(objective_func_vals)\n",
    "    plt.show()\n",
    "\n",
    "vqc = VQC(num_qubits=feature_dim,\n",
    "          feature_map=feature_map,\n",
    "          ansatz=ansatz,\n",
    "          loss=\"cross_entropy\",\n",
    "          optimizer=optimizer,\n",
    "          callback=callback_graph,\n",
    "          sampler=sampler)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Execute with Simulator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vqc.fit(norm_train_data, train_label_one_hot)\n",
    "\n",
    "train_score = vqc.score(norm_train_data, train_label_one_hot)\n",
    "test_score = vqc.score(norm_test_data, test_label_one_hot)\n",
    "\n",
    "print(f'--- Classification Train score: {train_score} ---')\n",
    "print(f'--- Classification Test score:  {test_score} ---')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Event Classfication using Quantum Kernel Method\n",
    "\n",
    "Now we try to tackle the same problme with quantum kernel method. First, implement a quantum circuit to encode input data as QuantumCircuit instance. You could use, e.g, ZFeatureMap or ZZFeatureMap as used above or something different.\n",
    "\n",
    "You can change the number of qubits used, but it appears that FidelityQuantumKernel class that we use later will work better if the number of input features is the same as the number of qubits."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "feature_map = ZZFeatureMap(feature_dimension=feature_dim, reps=1, entanglement='circular')\n",
    "feature_map.decompose().draw('mpl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next step is to create a quantum circuit to calculate kernel matrix from the feature map defined above. Qiskit has an API (FidelityQuantumKernel class) to do that, and we use it here."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# FidelityQuantumKernel creates a Sampler instance internally\n",
    "q_kernel = FidelityQuantumKernel(feature_map=feature_map)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We try to calculate the fidelity between the two inputs states, $|\\langle0^{\\otimes n}|U_{\\text{in}}^\\dagger(x_1)U_{\\text{in}}(x_0)|0^{\\otimes n}\\rangle|^2$, with the FidelityQuantumKernel class. The circuit to do that is obtained by assigning input data to parameters in the feature_map."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bind_params = dict(zip(feature_map.parameters, norm_train_data[0]))\n",
    "feature_map_0 = feature_map.assign_parameters(bind_params)\n",
    "bind_params = dict(zip(feature_map.parameters, norm_train_data[1]))\n",
    "feature_map_1 = feature_map.assign_parameters(bind_params)\n",
    "\n",
    "qc_circuit = q_kernel.fidelity.create_fidelity_circuit(feature_map_0, feature_map_1)\n",
    "qc_circuit.decompose().decompose().draw('mpl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Execute the circuit with simulator and calculate the probability of measuring 0 in all qubits after applying $U_{\\text{in}}^\\dagger(x_1)U_{\\text{in}}(x_0)$ to the input $|0^{\\otimes n}\\rangle$ state."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sampler = Sampler()\n",
    "\n",
    "job = sampler.run(qc_circuit, shots=10000)\n",
    "\n",
    "# quasi_dists[0] is the probability distribution of expected measured counts\n",
    "fidelity = job.result().quasi_dists[0].get(0, 0.)\n",
    "print(f'|<φ(x_1)|φ(x_0)>|^2 = {fidelity}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The FidelityQuantumKernel allows us to visualize a kernel matrix obtained from the fidelity calculations. Here we make plots of the kernel matrices calculated from the training data alone, and from the training and test data.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "matrix_train = q_kernel.evaluate(x_vec=norm_train_data)\n",
    "matrix_test = q_kernel.evaluate(x_vec=norm_test_data, y_vec=norm_train_data)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].imshow(np.asmatrix(matrix_train), interpolation='nearest', origin='upper', cmap='Blues')\n",
    "axs[0].set_title(\"training kernel matrix\")\n",
    "axs[1].imshow(np.asmatrix(matrix_test), interpolation='nearest', origin='upper', cmap='Reds')\n",
    "axs[1].set_title(\"validation kernel matrix\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, the data are classified into signal and background using support vector machine implemented in sklearn package."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qc_svc = SVC(kernel='precomputed') # Default value of hyperparameter (C) is 1\n",
    "qc_svc.fit(matrix_train, train_label)\n",
    "\n",
    "train_score = qc_svc.score(matrix_train, train_label)\n",
    "test_score = qc_svc.score(matrix_test, test_label)\n",
    "\n",
    "print(f'Precomputed kernel: Classification Train score: {train_score*100}%')\n",
    "print(f'Precomputed kernel: Classification Test score:  {test_score*100}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quantum Machine Learning on Quantum Data\n",
    "\n",
    "We have considered so far a classical ML task with input classical data. Next, we consider a bit more *quantum* ML task, in which quantum computer could be more advantageous than classical computer in the future.\n",
    "\n",
    "What we aim here is a *Hamiltonian learning* task of estimating classical parameters of a given Hamiltonian from quantum states generated under the Hamiltonian. The transverse-field Ising model is considered, and the tasks is to estimate the strength of transverse field by learning VQE-generated ground states using quantum neural networks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's begin VQE using the Ising-model Hamiltonian with varied transverse-field strengths."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from qiskit_algorithms.optimizers import SLSQP\n",
    "from qiskit.circuit.library import TwoLocal\n",
    "\n",
    "# VQE setup\n",
    "num_qubits = 2\n",
    "ansatz = TwoLocal(num_qubits, \"ry\", \"cz\", reps=3)  # Ry gates with trainable parameters and CZ for entanglement\n",
    "optimizer = SLSQP(maxiter=1000)  # Classical optimizer\n",
    "ansatz.decompose().draw('mpl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Ising model we consider has a parameter `alpha` that controls the mixture of transverse and longitudinal fields. `alpha=0` corresponds to a pure transverse field and `alpha=pi/2` a pure longitudinal field."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use Estimator\n",
    "estimator = Estimator()\n",
    "vqe = VQE(estimator, ansatz, optimizer)\n",
    "\n",
    "# Ising model Hamiltonian with transverse and longitudinal fields\n",
    "def get_hamiltonian(L, J, h, alpha=0):\n",
    "\n",
    "    # List of Hamiltonian terms as 3-tuples containing\n",
    "    # (1) the Pauli string,\n",
    "    # (2) the qubit indices corresponding to the Pauli string,\n",
    "    # (3) the coefficient.\n",
    "    ZZ_tuples = [(\"ZZ\", [i, i + 1], -J) for i in range(0, L - 1)]\n",
    "    Z_tuples = [(\"Z\", [i], -h * np.sin(alpha)) for i in range(0, L)]\n",
    "    X_tuples = [(\"X\", [i], -h * np.cos(alpha)) for i in range(0, L)]\n",
    "\n",
    "    # We create the Hamiltonian as a SparsePauliOp, via the method\n",
    "    # `from_sparse_list`, and multiply by the interaction term.\n",
    "    hamiltonian = SparsePauliOp.from_sparse_list([*ZZ_tuples, *Z_tuples, *X_tuples], num_qubits=L)\n",
    "    return hamiltonian.simplify()\n",
    "\n",
    "# Example:\n",
    "J = 0.2\n",
    "h = 1.2\n",
    "alpha = np.pi/8\n",
    "H = get_hamiltonian(L=num_qubits, J=J, h=h, alpha=alpha)\n",
    "\n",
    "result = vqe.compute_minimum_eigenvalue(H)\n",
    "#print(result)\n",
    "print(f'VQE energy value = {result.optimal_value:.5f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the system is small, the exact ground state energy can be calcualted by diagonalizing the Hamiltonian."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numpy_solver = NumPyMinimumEigensolver()\n",
    "result = numpy_solver.compute_minimum_eigenvalue(operator=H)\n",
    "ref_value = result.eigenvalue.real\n",
    "print(f\"Reference energy value = {ref_value:.5f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "OK, now create a dataset of VQE ground states with different `h` values. For simplicity, the `alpha` parameter is set to 0 and only the transverse field is present."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "from qiskit.circuit.library import RealAmplitudes\n",
    "from qiskit_machine_learning.algorithms import NeuralNetworkRegressor\n",
    "\n",
    "Nexp = 20   # number of experiments\n",
    "Ntrain = 20  # number of training data per experiment\n",
    "Ntest = 5\n",
    "J = 0.2\n",
    "alpha = 0\n",
    "\n",
    "#h_list = [np.random.rand()*0.6+0.7 for _ in range(Ntrain)]\n",
    "h_list = [np.random.rand() for _ in range(Ntrain)]\n",
    "print(f'Input field strenghs = {h_list}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_qubits = 2\n",
    "vqe_ansatz = TwoLocal(num_qubits, \"ry\", \"cz\", parameter_prefix='x')\n",
    "optimizer = SLSQP(maxiter=1000)\n",
    "\n",
    "estimator = Estimator()\n",
    "vqe = VQE(estimator, vqe_ansatz, optimizer)\n",
    "\n",
    "opt_vqe_energy = []\n",
    "opt_vqe_params = []\n",
    "for i in range(Ntrain):\n",
    "    H = get_hamiltonian(L=num_qubits, J=J, h=h_list[i], alpha=alpha)\n",
    "    result_vqe = vqe.compute_minimum_eigenvalue(H)\n",
    "    opt_vqe_energy.append(result_vqe.optimal_value)\n",
    "    opt_vqe_params.append(list(result_vqe.optimal_parameters.values()))\n",
    "    print('VQE i =',i)\n",
    "\n",
    "for i in range(Ntrain):\n",
    "    print(f'VQE[{i}] energy value = {opt_vqe_energy[i]:.5f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sanity check with exact diagonalizatiion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numpy_solver = NumPyMinimumEigensolver()\n",
    "for i in range(Ntrain):\n",
    "    H = get_hamiltonian(L=num_qubits, J=J, h=h_list[i], alpha=alpha)\n",
    "    result = numpy_solver.compute_minimum_eigenvalue(operator=H)\n",
    "    ref_value = result.eigenvalue.real\n",
    "    print(f\"Reference{i} energy value = {ref_value:.5f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now construct a quantum circuit with the optimized VQE circuit for generating ground states at the beginning and a QNN ansatz for learning the state at the end."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from qiskit_machine_learning.circuit.library import QNNCircuit\n",
    "\n",
    "# Observable : Pauli Z for the first qubit\n",
    "obs = SparsePauliOp(['IZ'],coeffs=2.)\n",
    "\n",
    "# Callback function to store loss values\n",
    "def callback_graph(weights, obj_func_eval):\n",
    "    objective_func_vals.append(obj_func_eval)\n",
    "\n",
    "# QNN setup with RealAmplitudes ansatz\n",
    "nlayer = 3  # number of CX-RY layers\n",
    "\n",
    "result_exp = []\n",
    "regressor_exp = []\n",
    "objective_func_vals_exp = []\n",
    "\n",
    "# Repeat Nexp times\n",
    "for iexp in range(Nexp):\n",
    "\n",
    "    qnn_ansatz = RealAmplitudes(num_qubits=num_qubits, reps=nlayer, parameter_prefix='theta')\n",
    "\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    qc.compose(vqe_ansatz, inplace=True)\n",
    "    qc.compose(qnn_ansatz, inplace=True)\n",
    "\n",
    "    # Random initial parameters within [0, pi]\n",
    "    initial_weights = np.random.rand((nlayer+1)*num_qubits)*np.pi\n",
    "\n",
    "    # Use EstimatorQNN class\n",
    "    qnn = EstimatorQNN(\n",
    "        circuit = qc,\n",
    "        input_params = opt_vqe_params[0],\n",
    "        weight_params = initial_weights,\n",
    "        observables = obs\n",
    "    )\n",
    "\n",
    "    # Use NeuralNetworkRegressor for regression task with COBYLA optimizer\n",
    "    regressor = NeuralNetworkRegressor(\n",
    "        neural_network = qnn,\n",
    "        loss = \"squared_error\",\n",
    "        optimizer = COBYLA(maxiter=300, tol=0.001),\n",
    "        callback = callback_graph\n",
    "    )\n",
    "\n",
    "    objective_func_vals = []\n",
    "    result_regres = regressor.fit(np.array(opt_vqe_params),np.array(h_list))\n",
    "    result_exp.append(result_regres)\n",
    "    regressor_exp.append(regressor)\n",
    "    objective_func_vals_exp.append(objective_func_vals)\n",
    "\n",
    "    print('iexp = ',iexp)\n",
    "    #print(f'iexp={iexp}, initial weights = {initial_weights}, final weights = {result_exp[iexp]._fit_result.x}')\n",
    "    #print(f'h_list = {h_list}, pred = {regressor_exp[iexp].predict(np.array(opt_vqe_params))}')\n",
    "\n",
    "#print(f'objective_func_vals_exp[0] ={objective_func_vals_exp[0]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make the plot of loss function values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(facecolor=\"w\")\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.title('Objective function value against iteration')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Objective function value\")\n",
    "for iexp in range(Nexp):\n",
    "    plt.plot(range(len(objective_func_vals_exp[iexp])), objective_func_vals_exp[iexp])\n",
    "#plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "See the correlation between the true and predicted transverse-field strengths from the optimized QNN model. The plot shows only the training data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_pred_exp = []\n",
    "for iexp in range(Nexp):\n",
    "    train_pred = regressor_exp[iexp].predict(np.array(opt_vqe_params))\n",
    "    train_pred_exp.append(train_pred)\n",
    "    plt.scatter(h_list, train_pred, label='training')\n",
    "plt.title('True vs Predicted values')\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.xlim(-0.2,1.2)\n",
    "plt.ylim(-0.5,1.5)\n",
    "#plt.legend()\n",
    "plt.plot([-0.2,1.2],[-0.2,1.2],'k--')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reconstruction of Charged Particles (Tracking)\n",
    "\n",
    "## Tracking with Annealing Technique\n",
    "\n",
    "In this exercise, we try to find tracks from detector hits using simulated annealing technique. This is based on the <a href=\"https://github.com/derlin/hepqpr-qallse\" target=\"_blank\">hepqpr-qallse</a> framework developed by Lucy Linder and LBNL group.\n",
    "\n",
    "First, let us import necessary modules."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "repo_dir = os.path.join(os.environ['HOME'], 'kmi-school-2024')\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "if not os.path.exists(os.path.join(repo_dir, 'qc_workbook', 'hepqpr')):\n",
    "    import subprocess\n",
    "\n",
    "    proc = subprocess.Popen(['git', 'submodule', 'init'], cwd=repo_dir)\n",
    "    proc.wait()\n",
    "\n",
    "    proc = subprocess.Popen(['git', 'submodule', 'update'], cwd=repo_dir)\n",
    "    proc.wait()\n",
    "\n",
    "    #os.symlink(os.path.join(repo_dir, 'hepqpr-qallse', 'src', 'hepqpr'), os.path.join(repo_dir, 'qc_workbook', 'hepqpr'))\n",
    "\n",
    "    %pip install 'git+https://github.com/LAL/trackml-library.git'\n",
    "    %pip install dwave-qbsolv\n",
    "    %pip install dwave-neal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, create a dataset of detector hits. The parameter `density` controls how many particles in an event are included in the dataset. The density = 0.0015 means the particle density corresponds to about 0.15% of a typical HL-LHC collision event.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from hepqpr.qallse.dsmaker import create_dataset\n",
    "\n",
    "density = 0.0015\n",
    "\n",
    "output_path = os.getcwd()+'/ds'\n",
    "prefix = 'ds'+str(density)\n",
    "\n",
    "metadata, path = create_dataset(\n",
    "    density=density,\n",
    "    output_path=output_path,\n",
    "    prefix=prefix,\n",
    "    gen_doublets=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will see that the dataset is created under ds in your working directory.\n",
    "\n",
    "Next, QUBO is produced from the dataset by reconstructing doublets, triplets and quadruplets from the hits and checking their relative geometries.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from hepqpr.qallse import *\n",
    "\n",
    "# ==== BUILD CONFIG\n",
    "loglevel = logging.INFO\n",
    "\n",
    "input_path = os.getcwd()+'/ds/'+prefix+'/event000001000-hits.csv'\n",
    "output_path = os.getcwd()+'/ds/'+prefix+'/'\n",
    "\n",
    "model_class = QallseD0  # model class to use\n",
    "extra_config = dict()  # model config\n",
    "\n",
    "dump_config = dict(\n",
    "    output_path = os.getcwd()+'/ds/'+prefix+'/',\n",
    "    prefix=prefix+'_',\n",
    "    xplets_kwargs=dict(format='json', indent=3), # use json (vs \"pickle\") and indent the output\n",
    "    qubo_kwargs=dict(w_marker=None, c_marker=None) # save the real coefficients VS generic placeholders\n",
    ")\n",
    "\n",
    "# ==== configure logging\n",
    "logging.basicConfig(\n",
    "    stream=sys.stderr,\n",
    "    format=\"%(asctime)s.%(msecs)03d [%(name)-15s %(levelname)-5s] %(message)s\",\n",
    "    datefmt='%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "logging.getLogger('hepqpr').setLevel(loglevel)\n",
    "\n",
    "# ==== build model\n",
    "# load data\n",
    "dw = DataWrapper.from_path(input_path)\n",
    "doublets = pd.read_csv(input_path.replace('-hits.csv', '-doublets.csv'))\n",
    "\n",
    "# build model\n",
    "model = model_class(dw, **extra_config)\n",
    "model.build_model(doublets)\n",
    "\n",
    "# dump model to a file\n",
    "dumper.dump_model(model, **dump_config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set up the annealing job by loading QUBO."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join as path_join\n",
    "\n",
    "from hepqpr.qallse.other.stdout_redirect import capture_stdout\n",
    "from hepqpr.qallse.other.dw_timing_recorder import solver_with_timing, TimingRecord\n",
    "from hepqpr.qallse.plotting import *\n",
    "\n",
    "\n",
    "# ==== RUN CONFIG\n",
    "nreads = 10\n",
    "nseed = 1000000\n",
    "\n",
    "loglevel = logging.INFO\n",
    "\n",
    "input_path = os.getcwd()+'/ds/'+prefix+'/event000001000-hits.csv'\n",
    "qubo_path = os.getcwd()+'/ds/'+prefix+'/'\n",
    "\n",
    "# ==== configure logging\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    format=\"%(asctime)s.%(msecs)03d [%(name)-15s %(levelname)-5s] %(message)s\",\n",
    "    datefmt='%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "logging.getLogger('hepqpr').setLevel(loglevel)\n",
    "\n",
    "# ==== build model\n",
    "# load data\n",
    "dw = DataWrapper.from_path(input_path)\n",
    "pickle_file = prefix+'_qubo.pickle'\n",
    "with open(path_join(qubo_path, pickle_file), 'rb') as f:\n",
    "    Q = pickle.load(f)\n",
    "#print(Q)\n",
    "\n",
    "import time\n",
    "start_time = time.process_time()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform simulated annealing using neal software.\n",
    "\n",
    "If it's successfully done, you can see a file \"plot_ds..._tracks_found.html\" in created in your working directory. This plot displays the detector hits in QUBO, projected onto a plane perpendicular to the beam axis, and shows which detector hits are successfully selected to from reconstructed tracks. The green lines correspond to reconstructed tracks, the blue lines missing tracks (i.e, tracks that were not reconstructed) and the red ones fake tracks (i.e, misreconstructed tracks that do not mach truth particles)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sample qubo\n",
    "\n",
    "# --- neal\n",
    "import neal\n",
    "sampler = neal.SimulatedAnnealingSampler()\n",
    "#import dimod\n",
    "#sampler = dimod.RandomSampler()\n",
    "response = sampler.sample_qubo(Q, num_reads=nreads, seed=nseed)\n",
    "\n",
    "exec_time = time.process_time() - start_time\n",
    "print(f'QUBO of size {len(Q)} sampled in {exec_time:.2f}s (NEAL).')\n",
    "print('')\n",
    "\n",
    "\n",
    "# get the results\n",
    "all_doublets = Qallse.process_sample(next(response.samples()))\n",
    "final_tracks, final_doublets = TrackRecreaterD().process_results(all_doublets)\n",
    "\n",
    "# compute stats\n",
    "en0 = dw.compute_energy(Q)\n",
    "en = response.record.energy[0]\n",
    "occs = response.record.num_occurrences\n",
    "\n",
    "p, r, ms = dw.compute_score(final_doublets)\n",
    "trackml_score = dw.compute_trackml_score(final_tracks)\n",
    "\n",
    "# print stats\n",
    "print(f'SAMPLE -- energy: {en:.4f}, ideal: {en0:.4f} (diff: {en-en0:.6f})')\n",
    "print(f'          best sample occurrence: {occs[0]}/{occs.sum()}')\n",
    "\n",
    "print(f'SCORE  -- precision (%): {p * 100}, recall (%): {r * 100}, missing: {len(ms)}')\n",
    "print(f'          tracks found: {len(final_tracks)}, trackml score (%): {trackml_score * 100}')\n",
    "\n",
    "# plotting examples\n",
    "dims = ['x', 'y']\n",
    "dout = 'plot_'+prefix+'_tracks_found.html'\n",
    "iplot_results(dw, final_doublets, ms, dims=dims, filename=dout)\n",
    "#iplot_results_tracks(dw, final_tracks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attention\n",
    "**Below we try to reconstruct tracks with VQE in a quantum circuit model. Since each segment is assigned to a qubit in this (naive) approach, the memory consumption quickly becomes explosive with the number of segments and the kernel will crash if the number of segments is larger than about 30 or so. Before proceeding, please make sure that the number of segments has to be less than 20-25.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hamiltonian Formulation and VQE Implementation\n",
    "\n",
    "In order to use VQE for optimization, the problem will need to be formulated in the form of Hamiltonian. If the problem is formulated such that the solution corresponds to the lowest energy state of the Hamiltonian, the VQE could solve the problem by finding such state.\n",
    "\n",
    "### QUBO Format\n",
    "\n",
    "Under this setup, the next step is whether a given segment is adopted as part of particle tracks or rejected as fake. In a sample of $N$ segments, the adoptation or rejection of $i$-th segment is associated to 1 or 0 of a binary variable $T_i$, and the variable $T_i$ is determined such that the objective function defined as\n",
    "\n",
    "$$\n",
    "O(b, T) = \\sum_{i=1}^N a_{i} T_i + \\sum_{i=1}^N \\sum_{j<i}^N b_{ij} T_i T_j\n",
    "$$\n",
    "\n",
    "is minimized. Here $a_i$ is the score of $i$-th segment and $b_{ij}$ is the score of the pair of $i$- and $j$-th segments. The objective function becomes smaller by selecting segments that have smaller $a_i$ values (pointing towards the detector center) and are paired with other segments with smaller $b_{ij}$ values (more consistent with a real track) and rejecting otherwise. Once the correct segments are identified, the corresponding tracks can be reconstructed with high efficiency. Therefore, solving this minimization problem is the key to tracking.\n",
    "\n",
    "Let us first extract the scores $a_i$ and $b_{ij}$ from the QUBO produced above (corresponding to the variable Q).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_max = 100\n",
    "\n",
    "nvar = 0\n",
    "key_i = []\n",
    "a_score = np.zeros(n_max)\n",
    "for (k1, k2), v in Q.items():\n",
    "    if k1 == k2:\n",
    "        a_score[nvar] = v\n",
    "        key_i.append(k1)\n",
    "        nvar += 1\n",
    "a_score = a_score[:nvar]\n",
    "\n",
    "b_score = np.zeros((n_max,n_max))\n",
    "for (k1, k2), v in Q.items():\n",
    "    if k1 != k2:\n",
    "        for i in range(nvar):\n",
    "            for j in range(nvar):\n",
    "                if k1 == key_i[i] and k2 == key_i[j]:\n",
    "                    if i < j:\n",
    "                        b_score[j][i] = v\n",
    "                    else:\n",
    "                        b_score[i][j] = v\n",
    "\n",
    "b_score = b_score[:nvar,:nvar]\n",
    "\n",
    "print(f'# of Segments: {nvar}')\n",
    "# Print out the first 5x5\n",
    "print(a_score[:5])\n",
    "print(b_score[:5, :5])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ising Format\n",
    "\n",
    "The QUBO objective function is not the form of Hamiltonian (i.e, not Hermitian operator). Therefore, the objective function needs to be transformed before solving with VQE. Given that $T_i$ takes a binary value $\\{0, 1\\}$, a new variable $s_i$ with values of $\\{+1, -1\\}$ can be defined by\n",
    "\n",
    "$$\n",
    "T_i = \\frac{1}{2} (1 - s_i).\n",
    "$$\n",
    "\n",
    "Note that $\\{+1, -1\\}$ is the eigenvalue of Pauli operator. By replacing $s_i$ with Pauli $Z$ operator acting on $i$-th qubit, we can obtain the following Hamiltonian for which the computational basis states in $N$-qubit system correspond to the eigenstates that encode adoptation or rejection of the segments:\n",
    "\n",
    "$$\n",
    "H(h, J, s) = \\sum_{i=1}^N h_i Z_i + \\sum_{i=1}^N \\sum_{j<i}^N J_{ij} Z_i Z_j + \\text{(constant)}\n",
    "$$\n",
    "\n",
    "The form of this Hamiltonian is the same as Ising model Hamiltonian, which often appears in various fields of natural science. The $\\text{constant}$ is a constant term and has no impact in variational method, hence ignored in the rest of this exercise."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise\n",
    "\n",
    "By following the above prescription, please calculate the coefficients $h_i$ and $J_{ij}$ of the Hamiltonian in the next cell."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_qubits = nvar\n",
    "\n",
    "coeff_h = np.zeros(num_qubits)\n",
    "coeff_J = np.zeros((num_qubits, num_qubits))\n",
    "\n",
    "# Calculate coeff_h and coeff_J from a_score and b_score\n",
    "coeff_h = -(a_score / 2. + (np.sum(b_score, axis=0) + np.sum(b_score, axis=1)) / 4.)\n",
    "coeff_J = b_score / 4."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, let us define the Hamiltonian used in VQE as a SparsePauliOp object. In {ref}`vqe_imp` the SparsePauliOp was used to define a single Pauli string $ZXY$, but the same class can be used for the sum of Pauli strings. For example,\n",
    "\n",
    "$$\n",
    "H = 0.2 IIZ + 0.3 ZZI + 0.1 ZIZ\n",
    "$$\n",
    "\n",
    "can be expressed as\n",
    "\n",
    "```python\n",
    "H = SparsePauliOp(['IIZ', 'ZZI', 'ZIZ'], coeffs=[0.2, 0.3, 0.1])\n",
    "```\n",
    "\n",
    "Note that the qubits are ordered from right to left (the most right operator acts on the 0-th qubit) according to the rule in Qiskit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise\n",
    "\n",
    "Pick up all the Pauli strings with non-zero coefficients and make the array of corresponding coefficients."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pauli_products = []\n",
    "coeffs = []\n",
    "\n",
    "for iq in range(num_qubits):\n",
    "    if np.isclose(coeff_h[iq], 0.):\n",
    "        continue\n",
    "\n",
    "    pauli_products.append(('I' * (num_qubits - iq - 1)) + 'Z' + ('I' * iq))\n",
    "    coeffs.append(coeff_h[iq])\n",
    "\n",
    "for iq in range(num_qubits):\n",
    "    for jq in range(iq):\n",
    "        if np.isclose(coeff_J[iq, jq], 0.):\n",
    "            continue\n",
    "\n",
    "        pauli = 'I' * (num_qubits - iq - 1)\n",
    "        pauli += 'Z'\n",
    "        pauli += 'I' * (iq - jq - 1)\n",
    "        pauli += 'Z'\n",
    "        pauli += 'I' * jq\n",
    "        pauli_products.append(pauli)\n",
    "\n",
    "        coeffs.append(coeff_J[iq, jq])\n",
    "\n",
    "\n",
    "hamiltonian = SparsePauliOp(pauli_products, coeffs=coeffs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Executing VQE\n",
    "\n",
    "Now we try to approximately obtain the lowest energy eigenvalues using VQE with the Hamiltonian defined above. But, before doing that, let us diagonalize the Hamiltonian matrix and calculate the exact energy eigenvalues and eigenstates."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Diagonalize the Hamiltonian and calculate the energy eigenvalues and eigenstates\n",
    "ee = NumPyMinimumEigensolver()\n",
    "result_diag = ee.compute_minimum_eigenvalue(hamiltonian)\n",
    "\n",
    "# Print out the combination of qubits corresponding to the lowest energy\n",
    "print(f'Minimum eigenvalue (diagonalization): {result_diag.eigenvalue.real}')\n",
    "# Expand the state with computational bases and select the one with the highest probability\n",
    "optimal_segments_diag = OptimizationApplication.sample_most_likely(result_diag.eigenstate)\n",
    "print(f'Optimal segments (diagonalization): {optimal_segments_diag}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we move to VQE."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "backend = AerSimulator()\n",
    "# Create Estimator instance\n",
    "estimator = BackendEstimator(backend)\n",
    "\n",
    "# Define variational form of VQE using a built-in function called TwoLocal.\n",
    "ansatz = TwoLocal(num_qubits, 'ry', 'cz', 'linear', reps=1)\n",
    "\n",
    "# Optimizer\n",
    "optimizer_name = 'SPSA'\n",
    "\n",
    "if optimizer_name == 'SPSA':\n",
    "    optimizer = SPSA(maxiter=300)\n",
    "    grad = ParamShiftEstimatorGradient(estimator)\n",
    "\n",
    "elif optimizer_name == 'COBYLA':\n",
    "    optimizer = COBYLA(maxiter=500)\n",
    "    grad = None\n",
    "\n",
    "# Initialize parameters with random values\n",
    "rng = np.random.default_rng()\n",
    "init = rng.uniform(0., 2. * np.pi, size=len(ansatz.parameters))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make VQE object and search for the ground state\n",
    "vqe = VQE(estimator, ansatz, optimizer, gradient=grad, initial_point=init)\n",
    "result_vqe = vqe.compute_minimum_eigenvalue(hamiltonian)\n",
    "\n",
    "# Create state vector from the ansatz using optimized parameters\n",
    "optimal_state = Statevector(ansatz.assign_parameters(result_vqe.optimal_parameters))\n",
    "\n",
    "# Print out the combination of qubits with the lowest energy\n",
    "print(f'Minimum eigenvalue (VQE): {result_vqe.eigenvalue.real}')\n",
    "optimal_segments_vqe = OptimizationApplication.sample_most_likely(optimal_state)\n",
    "print(f'Optimal segments (VQE): {optimal_segments_vqe}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check again reconstructed tracks in the detector plane for fun. You can switch the results of exact diagonalization and VQE with the variable `type`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from hepqpr.qallse import DataWrapper, Qallse, TrackRecreaterD\n",
    "from hepqpr.qallse.plotting import iplot_results, iplot_results_tracks\n",
    "from hepqpr.qallse.utils import diff_rows\n",
    "\n",
    "# Results tp show: diag = Exact diagonalization, vqe = VQE\n",
    "type = \"diag\"\n",
    "#type = \"vqe\"\n",
    "\n",
    "if type == \"diag\":\n",
    "    optimal_segments = optimal_segments_diag\n",
    "elif type == \"vqe\":\n",
    "    optimal_segments = optimal_segments_vqe\n",
    "\n",
    "samples = dict(zip(key_i, optimal_segments))\n",
    "\n",
    "# get the results\n",
    "all_doublets = Qallse.process_sample(samples)\n",
    "\n",
    "final_tracks, final_doublets = TrackRecreaterD().process_results(all_doublets)\n",
    "\n",
    "#dw = DataWrapper.from_path('data/event000001000-hits.csv')\n",
    "input_path = os.getcwd()+'/ds/'+prefix+'/event000001000-hits.csv'\n",
    "dw = DataWrapper.from_path(input_path)\n",
    "\n",
    "p, r, ms = dw.compute_score(final_doublets)\n",
    "trackml_score = dw.compute_trackml_score(final_tracks)\n",
    "\n",
    "print(f'SCORE  -- precision (%): {p * 100}, recall (%): {r * 100}, missing: {len(ms)}')\n",
    "print(f'          tracks found: {len(final_tracks)}, trackml score (%): {trackml_score * 100}')\n",
    "\n",
    "dims = ['x', 'y']\n",
    "_, missings, _ = diff_rows(final_doublets, dw.get_real_doublets())\n",
    "dout = 'plot-ising_'+type+'_found_tracks.html'\n",
    "iplot_results(dw, final_doublets, missings, dims=dims, filename=dout)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}