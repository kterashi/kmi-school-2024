{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Quantum Machine Learning Application to Classification of New Physics Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quantum Neural Networks\n",
    "\n",
    "### Preparation of training data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'QuantumCircuit' from 'qiskit' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m accuracy_score\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OrderedDict\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mqiskit\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m QuantumCircuit, transpile\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mqiskit\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcircuit\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Parameter, ParameterVector\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mqiskit\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcircuit\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlibrary\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TwoLocal, ZFeatureMap, ZZFeatureMap\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'QuantumCircuit' from 'qiskit' (unknown location)"
     ]
    }
   ],
   "source": [
    "# Tested with python 3.10.11, qiskit 0.42.1, numpy 1.23.5, scipy 1.9.3\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import OrderedDict\n",
    "\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit.circuit import Parameter, ParameterVector\n",
    "from qiskit.circuit.library import TwoLocal, ZFeatureMap, ZZFeatureMap\n",
    "from qiskit.primitives import Estimator, Sampler, BackendEstimator\n",
    "from qiskit.quantum_info import SparsePauliOp, Statevector\n",
    "from qiskit_algorithms.gradients import ParamShiftEstimatorGradient\n",
    "from qiskit_algorithms.minimum_eigensolvers import VQE, NumPyMinimumEigensolver\n",
    "from qiskit_algorithms.optimizers import SPSA, COBYLA\n",
    "from qiskit_optimization.applications import OptimizationApplication\n",
    "from qiskit_ibm_runtime import Session, Sampler as RuntimeSampler\n",
    "from qiskit_ibm_runtime.accounts import AccountNotFoundError\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit_machine_learning.algorithms.classifiers import VQC\n",
    "from qiskit_machine_learning.kernels import FidelityQuantumKernel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read data from file\n",
    "df = pd.read_csv(\"data/SUSY_1K.csv\",\n",
    "                 names=('isSignal','lep1_pt','lep1_eta','lep1_phi','lep2_pt','lep2_eta',\n",
    "                        'lep2_phi','miss_ene','miss_phi','MET_rel','axial_MET','M_R','M_TR_2',\n",
    "                        'R','MT2','S_R','M_Delta_R','dPhi_r_b','cos_theta_r1'))\n",
    "\n",
    "# Number of input features for training\n",
    "feature_dim = 3\n",
    "\n",
    "# Feature variables\n",
    "if feature_dim == 3:\n",
    "    selected_features = ['lep1_pt', 'lep2_pt', 'miss_ene']\n",
    "elif feature_dim == 5:\n",
    "    selected_features = ['lep1_pt','lep2_pt','miss_ene','M_TR_2','M_Delta_R']\n",
    "elif feature_dim == 7:\n",
    "    selected_features = ['lep1_pt','lep1_eta','lep2_pt','lep2_eta','miss_ene','M_TR_2','M_Delta_R']\n",
    "\n",
    "# Number of events used in the training and testing\n",
    "train_size = 20\n",
    "test_size = 20\n",
    "\n",
    "df_sig = df.loc[df.isSignal==1, selected_features]\n",
    "df_bkg = df.loc[df.isSignal==0, selected_features]\n",
    "\n",
    "# Extract the samples\n",
    "df_sig_train = df_sig.values[:train_size]\n",
    "df_bkg_train = df_bkg.values[:train_size]\n",
    "df_sig_test = df_sig.values[train_size:train_size + test_size]\n",
    "df_bkg_test = df_bkg.values[train_size:train_size + test_size]\n",
    "# The first train_size events contain SUSY signal and the last train_size events do not.\n",
    "train_data = np.concatenate([df_sig_train, df_bkg_train])\n",
    "# The first test_size events contain SUSY signal and the last test_size events do not.\n",
    "test_data = np.concatenate([df_sig_test, df_bkg_test])\n",
    "\n",
    "# Label\n",
    "train_label = np.zeros(train_size * 2, dtype=int)\n",
    "train_label[:train_size] = 1\n",
    "test_label = np.zeros(train_size * 2, dtype=int)\n",
    "test_label[:test_size] = 1\n",
    "\n",
    "train_label_one_hot = np.zeros((train_size * 2, 2))\n",
    "train_label_one_hot[:train_size, 0] = 1\n",
    "train_label_one_hot[train_size:, 1] = 1\n",
    "test_label_one_hot = np.zeros((test_size * 2, 2))\n",
    "test_label_one_hot[:test_size, 0] = 1\n",
    "test_label_one_hot[test_size:, 1] = 1\n",
    "\n",
    "#datapoints, class_to_label = split_dataset_to_data_and_labels(test_input)\n",
    "#datapoints_tr, class_to_label_tr = split_dataset_to_data_and_labels(training_input)\n",
    "\n",
    "mms = MinMaxScaler((-1, 1))\n",
    "norm_train_data = mms.fit_transform(train_data)\n",
    "norm_test_data = mms.transform(test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### State Preparation with Feature Map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#feature_map = ZFeatureMap(feature_dimension=feature_dim, reps=1)\n",
    "feature_map = ZZFeatureMap(feature_dimension=feature_dim, reps=1, entanglement='circular')\n",
    "feature_map.decompose().draw('mpl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### State Transformation with Variational Form"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ansatz = TwoLocal(num_qubits=feature_dim, rotation_blocks=['ry', 'rz'], entanglement_blocks='cz', entanglement='circular', reps=3)\n",
    "#ansatz = TwoLocal(num_qubits=feature_dim, rotation_blocks=['ry'], entanglement_blocks='cz', entanglement='circular', reps=3)\n",
    "ansatz.decompose().draw('mpl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Measurement and Model Output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use Sampler instead of backend\n",
    "sampler = Sampler()\n",
    "\n",
    "# When using quantum hardware\n",
    "# instance = 'ibm-q/open/main'\n",
    "\n",
    "# try:\n",
    "#     service = QiskitRuntimeService(channel='ibm_quantum', instance=instance)\n",
    "# except AccountNotFoundError:\n",
    "#     service = QiskitRuntimeService(channel='ibm_quantum', token='__paste_your_token_here__',\n",
    "#                                    instance=instance)\n",
    "\n",
    "# backend_name = 'ibm_washington'\n",
    "# session = Session(service=service, backend=backend_name)\n",
    "\n",
    "# sampler = RuntimeSampler(session=session)\n",
    "\n",
    "maxiter = 300\n",
    "\n",
    "optimizer = COBYLA(maxiter=maxiter, disp=True)\n",
    "\n",
    "objective_func_vals = []\n",
    "# Draw the value of objective function every time when the fit() method is called\n",
    "def callback_graph(weights, obj_func_eval):\n",
    "    clear_output(wait=True)\n",
    "    objective_func_vals.append(obj_func_eval)\n",
    "    #print('obj_func_eval =',obj_func_eval)\n",
    "\n",
    "    plt.title(\"Objective function value against iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Objective function value\")\n",
    "    plt.plot(objective_func_vals)\n",
    "    plt.show()\n",
    "\n",
    "vqc = VQC(num_qubits=feature_dim,\n",
    "          feature_map=feature_map,\n",
    "          ansatz=ansatz,\n",
    "          loss=\"cross_entropy\",\n",
    "          optimizer=optimizer,\n",
    "          callback=callback_graph,\n",
    "          sampler=sampler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Execute with Simulator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vqc.fit(norm_train_data, train_label_one_hot)\n",
    "\n",
    "train_score = vqc.score(norm_train_data, train_label_one_hot)\n",
    "test_score = vqc.score(norm_test_data, test_label_one_hot)\n",
    "\n",
    "print(f'--- Classification Train score: {train_score} ---')\n",
    "print(f'--- Classification Test score:  {test_score} ---')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quantum Kernel Method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us first implement a quantum circuit to encode input data as QuantumCircuit instance. You could use, e.g, ZFeatureMap or ZZFeatureMap used abovem or write the circuit by hand by creating an empty QuantumCircuit object and using Parameter or ParameterVector.\n",
    "\n",
    "You can change the number of qubits used, but it appears that FidelityQuantumKernel class that we use later will work better if the number of input features is the same as the number of qubits."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "##################\n",
    "### EDIT BELOW ###\n",
    "##################\n",
    "\n",
    "#回路をスクラッチから書く場合\n",
    "input_features = ParameterVector('x', feature_dim)\n",
    "num_qubits = feature_dim\n",
    "feature_map = QuantumCircuit(num_qubits)\n",
    "# ...\n",
    "\n",
    "##################\n",
    "### EDIT ABOVE ###\n",
    "##################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next step is to create a quantum circuit named \"manual_kernel\", by which the kernel matrix is calculated using the feature map defined above. Qiskit has an API (FidelityQuantumKernel class) that does this automatically and we will use that next. Here you could consider constructing the circuit by starting with an empty QuantumCircuit object and placing the feature map defined above."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "manual_kernel = QuantumCircuit(feature_map.num_qubits)\n",
    "\n",
    "##################\n",
    "### EDIT BELOW ###\n",
    "##################\n",
    "\n",
    "##################\n",
    "### EDIT ABOVE ###\n",
    "##################\n",
    "\n",
    "manual_kernel.measure_all()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Execute the circuit with simulator and calculate the probability of measuring 0 in all qubits, $|\\langle0^{\\otimes n}|U_{\\text{in}}^\\dagger(x_1)U_{\\text{in}}(x_0)|0^{\\otimes n}\\rangle|^2$, that provides the kernel.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sampler = Sampler()\n",
    "\n",
    "first_two_inputs = np.concatenate(norm_train_data[:2]).flatten()\n",
    "\n",
    "job = sampler.run(manual_kernel, parameter_values=first_two_inputs, shots=10000)\n",
    "\n",
    "# quasi_dists[0] is the probability distribution expected from measured counts of the manual_kernel\n",
    "fidelity = job.result().quasi_dists[0].get(0, 0.)\n",
    "print(f'|<φ(x_0)|φ(x_1)>|^2 = {fidelity}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can do the same thing using FidelityQuantumKernel class."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# FidelityQuantumKernel creates a Sampler instance internally\n",
    "q_kernel = FidelityQuantumKernel(feature_map=feature_map)\n",
    "\n",
    "bind_params = dict(zip(feature_map.parameters, norm_train_data[0]))\n",
    "feature_map_0 = feature_map.assign_parameters(bind_params)\n",
    "bind_params = dict(zip(feature_map.parameters, norm_train_data[1]))\n",
    "feature_map_1 = feature_map.assign_parameters(bind_params)\n",
    "\n",
    "qc_circuit = q_kernel.fidelity.create_fidelity_circuit(feature_map_0, feature_map_1)\n",
    "qc_circuit.decompose().decompose().draw('mpl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "FidelityQuantumKernel allows you to visualize the kernel matrix. Here we make the plots of kernel matrix calculated only from the training data, and that from the training and test data.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "matrix_train = q_kernel.evaluate(x_vec=norm_train_data)\n",
    "matrix_test = q_kernel.evaluate(x_vec=norm_test_data, y_vec=norm_train_data)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].imshow(np.asmatrix(matrix_train), interpolation='nearest', origin='upper', cmap='Blues')\n",
    "axs[0].set_title(\"training kernel matrix\")\n",
    "axs[1].imshow(np.asmatrix(matrix_test), interpolation='nearest', origin='upper', cmap='Reds')\n",
    "axs[1].set_title(\"validation kernel matrix\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, the data are classified into signal and background using support vector machine implemented in sklearn package."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qc_svc = SVC(kernel='precomputed') # Default value of hyperparameter (C) is 1\n",
    "qc_svc.fit(matrix_train, train_label)\n",
    "\n",
    "train_score = qc_svc.score(matrix_train, train_label)\n",
    "test_score = qc_svc.score(matrix_test, test_label)\n",
    "\n",
    "print(f'Precomputed kernel: Classification Train score: {train_score*100}%')\n",
    "print(f'Precomputed kernel: Classification Test score:  {test_score*100}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reconstruction of Charged Particles (Tracking)\n",
    "\n",
    "First, let us import necessary modules."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "repo_dir = os.path.join(os.environ['HOME'], 'kmi-school-2024')\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "if not os.path.exists(os.path.join(repo_dir, 'qc_workbook', 'hepqpr')):\n",
    "    import subprocess\n",
    "\n",
    "    proc = subprocess.Popen(['git', 'submodule', 'init'], cwd=repo_dir)\n",
    "    proc.wait()\n",
    "\n",
    "    proc = subprocess.Popen(['git', 'submodule', 'update'], cwd=repo_dir)\n",
    "    proc.wait()\n",
    "\n",
    "    #os.symlink(os.path.join(repo_dir, 'hepqpr-qallse', 'src', 'hepqpr'), os.path.join(repo_dir, 'qc_workbook', 'hepqpr'))\n",
    "\n",
    "    %pip install 'git+https://github.com/LAL/trackml-library.git'\n",
    "    %pip install dwave-qbsolv\n",
    "    %pip install dwave-neal"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sample_most_likely(state_vector):\n",
    "    \"\"\"Compute the most likely binary string from state vector.\n",
    "    Args:\n",
    "        state_vector (numpy.ndarray or dict): state vector or counts.\n",
    "    Returns:\n",
    "        numpy.ndarray: binary string as numpy.ndarray of ints.\n",
    "    \"\"\"\n",
    "    if isinstance(state_vector, (OrderedDict, dict)):\n",
    "        # get the binary string with the largest count\n",
    "        binary_string = sorted(state_vector.items(), key=lambda kv: kv[1])[-1][0]\n",
    "        x = np.asarray([int(y) for y in reversed(list(binary_string))])\n",
    "        return x\n",
    "    elif isinstance(state_vector, Statevector):\n",
    "        binary_string = list(state_vector.sample().keys())[0]\n",
    "        x = np.asarray([int(y) for y in reversed(list(binary_string))])\n",
    "        return x\n",
    "    else:\n",
    "        n = int(np.log2(state_vector.shape[0]))\n",
    "        k = np.argmax(np.abs(state_vector))\n",
    "        x = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            x[i] = k % 2\n",
    "            k >>= 1\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from hepqpr.qallse.dsmaker import create_dataset\n",
    "\n",
    "density = 0.0015\n",
    "output_path = os.getcwd()+'/ds'\n",
    "prefix = 'ds'+str(int(1000*density))\n",
    "\n",
    "metadata, path = create_dataset(\n",
    "    density=density,\n",
    "    output_path=output_path,\n",
    "    prefix=prefix,\n",
    "    gen_doublets=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from hepqpr.qallse import *\n",
    "\n",
    "# ==== BUILD CONFIG\n",
    "loglevel = logging.INFO\n",
    "\n",
    "input_path = os.getcwd()+'/ds/'+prefix+'/event000001000-hits.csv'\n",
    "output_path = os.getcwd()+'/ds/'+prefix+'/'\n",
    "\n",
    "model_class = QallseD0  # model class to use\n",
    "extra_config = dict()  # model config\n",
    "\n",
    "dump_config = dict(\n",
    "    output_path = os.getcwd()+'/ds/'+prefix+'/',\n",
    "    prefix=prefix+'_',\n",
    "    xplets_kwargs=dict(format='json', indent=3), # use json (vs \"pickle\") and indent the output\n",
    "    qubo_kwargs=dict(w_marker=None, c_marker=None) # save the real coefficients VS generic placeholders\n",
    ")\n",
    "\n",
    "# ==== configure logging\n",
    "logging.basicConfig(\n",
    "    stream=sys.stderr,\n",
    "    format=\"%(asctime)s.%(msecs)03d [%(name)-15s %(levelname)-5s] %(message)s\",\n",
    "    datefmt='%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "logging.getLogger('hepqpr').setLevel(loglevel)\n",
    "\n",
    "# ==== build model\n",
    "# load data\n",
    "dw = DataWrapper.from_path(input_path)\n",
    "doublets = pd.read_csv(input_path.replace('-hits.csv', '-doublets.csv'))\n",
    "\n",
    "# build model\n",
    "model = model_class(dw, **extra_config)\n",
    "model.build_model(doublets)\n",
    "\n",
    "# dump model to a file\n",
    "dumper.dump_model(model, **dump_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join as path_join\n",
    "\n",
    "from hepqpr.qallse.other.stdout_redirect import capture_stdout\n",
    "from hepqpr.qallse.other.dw_timing_recorder import solver_with_timing, TimingRecord\n",
    "from hepqpr.qallse.plotting import *\n",
    "\n",
    "\n",
    "# ==== RUN CONFIG\n",
    "nreads = 10\n",
    "nseed = 1000000\n",
    "\n",
    "loglevel = logging.INFO\n",
    "\n",
    "input_path = os.getcwd()+'/ds/'+prefix+'/event000001000-hits.csv'\n",
    "qubo_path = os.getcwd()+'/ds/'+prefix+'/'\n",
    "\n",
    "# ==== configure logging\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    format=\"%(asctime)s.%(msecs)03d [%(name)-15s %(levelname)-5s] %(message)s\",\n",
    "    datefmt='%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "logging.getLogger('hepqpr').setLevel(loglevel)\n",
    "\n",
    "# ==== build model\n",
    "# load data\n",
    "dw = DataWrapper.from_path(input_path)\n",
    "pickle_file = prefix+'_qubo.pickle'\n",
    "with open(path_join(qubo_path, pickle_file), 'rb') as f:\n",
    "    Q = pickle.load(f)\n",
    "#print(Q)\n",
    "\n",
    "import time\n",
    "start_time = time.process_time()\n",
    "\n",
    "# Sample qubo\n",
    "\n",
    "# --- neal\n",
    "import neal\n",
    "sampler = neal.SimulatedAnnealingSampler()\n",
    "#import dimod\n",
    "#sampler = dimod.RandomSampler()\n",
    "response = sampler.sample_qubo(Q, num_reads=nreads, seed=nseed)\n",
    "\n",
    "exec_time = time.process_time() - start_time\n",
    "print(f'QUBO of size {len(Q)} sampled in {exec_time:.2f}s (NEAL).')\n",
    "print('')\n",
    "\n",
    "\n",
    "# get the results\n",
    "all_doublets = Qallse.process_sample(next(response.samples()))\n",
    "final_tracks, final_doublets = TrackRecreaterD().process_results(all_doublets)\n",
    "\n",
    "# compute stats\n",
    "en0 = dw.compute_energy(Q)\n",
    "en = response.record.energy[0]\n",
    "occs = response.record.num_occurrences\n",
    "\n",
    "p, r, ms = dw.compute_score(final_doublets)\n",
    "trackml_score = dw.compute_trackml_score(final_tracks)\n",
    "\n",
    "# print stats\n",
    "print(f'SAMPLE -- energy: {en:.4f}, ideal: {en0:.4f} (diff: {en-en0:.6f})')\n",
    "print(f'          best sample occurrence: {occs[0]}/{occs.sum()}')\n",
    "\n",
    "print(f'SCORE  -- precision (%): {p * 100}, recall (%): {r * 100}, missing: {len(ms)}')\n",
    "print(f'          tracks found: {len(final_tracks)}, trackml score (%): {trackml_score * 100}')\n",
    "\n",
    "# plotting examples\n",
    "dims = ['x', 'y']\n",
    "dout = 'plot_'+prefix+'_tracks_found.html'\n",
    "iplot_results(dw, final_doublets, ms, dims=dims, filename=dout)\n",
    "#iplot_results_tracks(dw, final_tracks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hamiltonian Formulation and VQE Implementation\n",
    "\n",
    "In order to use VQE for optimization, the problem will need to be formulated in the form of Hamiltonian. If the problem is formulated such that the solution corresponds to the lowest energy state of the Hamiltonian, the VQE could solve the problem by finding such state.\n",
    "\n",
    "### QUBO Format\n",
    "\n",
    "Under this setup, the next step is whether a given segment is adopted as part of particle tracks or rejected as fake. In a sample of $N$ segments, the adoptation or rejection of $i$-th segment is associated to 1 or 0 of a binary variable $T_i$, and the variable $T_i$ is determined such that the objective function defined as\n",
    "\n",
    "$$\n",
    "O(b, T) = \\sum_{i=1}^N a_{i} T_i + \\sum_{i=1}^N \\sum_{j<i}^N b_{ij} T_i T_j\n",
    "$$\n",
    "\n",
    "is minimized. Here $a_i$ is the score of $i$-th segment and $b_{ij}$ is the score of the pair of $i$- and $j$-th segments. The objective function becomes smaller by selecting segments that have smaller $a_i$ values (pointing towards the detector center) and are paired with other segments with smaller $b_{ij}$ values (more consistent with a real track) and rejecting otherwise. Once correct segments are identified, the corresponding tracks can be reconstructed with high efficiency. Therefore, solving this minimization problem is the key to tracking.\n",
    "\n",
    "Let us first read out the scores $a_i$ and $b_{ij}$. The two scores are stored in a two-dimensional array `b`, and `b[i,i]` corresponds to $a_i$.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_max = 100\n",
    "\n",
    "nvar = 0\n",
    "key_i = []\n",
    "a_score = np.zeros(n_max)\n",
    "for (k1, k2), v in Q.items():\n",
    "    if k1 == k2:\n",
    "        a_score[nvar] = v\n",
    "        key_i.append(k1)\n",
    "        nvar += 1\n",
    "a_score = a_score[:nvar]\n",
    "\n",
    "b_score = np.zeros((n_max,n_max))\n",
    "for (k1, k2), v in Q.items():\n",
    "    if k1 != k2:\n",
    "        for i in range(nvar):\n",
    "            for j in range(nvar):\n",
    "                if k1 == key_i[i] and k2 == key_i[j]:\n",
    "                    if i < j:\n",
    "                        b_score[j][i] = v\n",
    "                    else:\n",
    "                        b_score[i][j] = v\n",
    "\n",
    "b_score = b_score[:nvar,:nvar]\n",
    "\n",
    "print(f'# of Segments: {nvar}')\n",
    "# Print out the first 5x5\n",
    "print(a_score[:5])\n",
    "print(b_score[:5, :5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ising Format\n",
    "\n",
    "The QUBO objective function is not the form of Hamiltonian (i.e, not Hermitian operator). Therefore, the objective function needs to be transformed before solving with VQE. Given that $T_i$ takes a binary value $\\{0, 1\\}$, a new variable $s_i$ with values of $\\{+1, -1\\}$ can be defined by\n",
    "\n",
    "$$\n",
    "T_i = \\frac{1}{2} (1 - s_i).\n",
    "$$\n",
    "\n",
    "Note that $\\{+1, -1\\}$ is the eigenvalue of Pauli operator. By replacing $s_i$ with Pauli $Z$ operator acting on $i$-th qubit, the following objective Hamiltonian for which computational basis states in $N$-qubit system correspond to the eigenstates that encode adoptation or rejection of the segments is obtained.\n",
    "\n",
    "$$\n",
    "H(h, J, s) = \\sum_{i=1}^N h_i Z_i + \\sum_{i=1}^N \\sum_{j<i}^N J_{ij} Z_i Z_j + \\text{(constant)}\n",
    "$$\n",
    "\n",
    "The form of this Hamiltonian is the same as Ising model Hamiltonian, which often appears in various fields of natural science. The $\\text{constant}$ is constant and has no impact in variational method, hence is ignored in the rest of this exercise."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise\n",
    "\n",
    "By following the above prescription, please calculate the coefficients $h_i$ and $J_{ij}$ of the Hamiltonian in the next cell."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_qubits = nvar\n",
    "\n",
    "coeff_h = np.zeros(num_qubits)\n",
    "coeff_J = np.zeros((num_qubits, num_qubits))\n",
    "\n",
    "##################\n",
    "### EDIT BELOW ###\n",
    "##################\n",
    "\n",
    "# Calculate coeff_h and coeff_J from a_score and b_score\n",
    "coeff_h = -(a_score / 2. + (np.sum(b_score, axis=0) + np.sum(b_score, axis=1)) / 4.)\n",
    "coeff_J = b_score / 4.\n",
    "\n",
    "##################\n",
    "### EDIT ABOVE ###\n",
    "##################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, let us define the Hamiltonian used in VQE as a SparsePauliOp object. In {ref}`vqe_imp` the SparsePauliOp was used to define a single Pauli string $ZXY$, but the same class can be used for the sum of Pauli strings. For example,\n",
    "\n",
    "$$\n",
    "H = 0.2 IIZ + 0.3 ZZI + 0.1 ZIZ\n",
    "$$\n",
    "\n",
    "can be expressed as\n",
    "\n",
    "```python\n",
    "H = SparsePauliOp(['IIZ', 'ZZI', 'ZIZ'], coeffs=[0.2, 0.3, 0.1])\n",
    "```\n",
    "\n",
    "Note that the qubits are ordered from right to left (the most right operator acts on the 0-th qubit) according to the rule in Qiskit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise\n",
    "\n",
    "Pick up all the Pauli strings with non-zero coefficients and make the array of corresponding coefficients."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pauli_products = []\n",
    "coeffs = []\n",
    "\n",
    "##################\n",
    "### EDIT BELOW ###\n",
    "##################\n",
    "\n",
    "for iq in range(num_qubits):\n",
    "    if np.isclose(coeff_h[iq], 0.):\n",
    "        continue\n",
    "\n",
    "    pauli_products.append(('I' * (num_qubits - iq - 1)) + 'Z' + ('I' * iq))\n",
    "    coeffs.append(coeff_h[iq])\n",
    "\n",
    "for iq in range(num_qubits):\n",
    "    for jq in range(iq):\n",
    "        if np.isclose(coeff_J[iq, jq], 0.):\n",
    "            continue\n",
    "\n",
    "        pauli = 'I' * (num_qubits - iq - 1)\n",
    "        pauli += 'Z'\n",
    "        pauli += 'I' * (iq - jq - 1)\n",
    "        pauli += 'Z'\n",
    "        pauli += 'I' * jq\n",
    "        pauli_products.append(pauli)\n",
    "\n",
    "        coeffs.append(coeff_J[iq, jq])\n",
    "\n",
    "##################\n",
    "### EDIT ABOVE ###\n",
    "##################\n",
    "\n",
    "hamiltonian = SparsePauliOp(pauli_products, coeffs=coeffs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Executing VQE\n",
    "\n",
    "Now we try to approximately obtain the lowest energy eigenvalues using VQE with the Hamiltonian defined above. But, before doing that, let us diagonalize the Hamiltonian matrix and calculate the exact energy eigenvalues and eigenstates."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Diagonalize the Hamiltonian and calculate the energy eigenvalues and eigenstates\n",
    "ee = NumPyMinimumEigensolver()\n",
    "result_diag = ee.compute_minimum_eigenvalue(hamiltonian)\n",
    "\n",
    "# Print out the combination of qubits corresponding to the lowest energy\n",
    "print(f'Minimum eigenvalue (diagonalization): {result_diag.eigenvalue.real}')\n",
    "# Expand the state with computational bases and select the one with the highest probability\n",
    "optimal_segments_diag = OptimizationApplication.sample_most_likely(result_diag.eigenstate)\n",
    "print(f'Optimal segments (diagonalization): {optimal_segments_diag}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "backend = AerSimulator()\n",
    "# Create Estimator instance\n",
    "estimator = BackendEstimator(backend)\n",
    "\n",
    "# Define variational form of VQE using a built-in function called TwoLocal.\n",
    "ansatz = TwoLocal(num_qubits, 'ry', 'cz', 'linear', reps=1)\n",
    "\n",
    "# Optimizer\n",
    "optimizer_name = 'SPSA'\n",
    "\n",
    "if optimizer_name == 'SPSA':\n",
    "    optimizer = SPSA(maxiter=300)\n",
    "    grad = ParamShiftEstimatorGradient(estimator)\n",
    "\n",
    "elif optimizer_name == 'COBYLA':\n",
    "    optimizer = COBYLA(maxiter=500)\n",
    "    grad = None\n",
    "\n",
    "# Initialize parameters with random values\n",
    "rng = np.random.default_rng()\n",
    "init = rng.uniform(0., 2. * np.pi, size=len(ansatz.parameters))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make VQE object and search for the ground state\n",
    "vqe = VQE(estimator, ansatz, optimizer, gradient=grad, initial_point=init)\n",
    "result_vqe = vqe.compute_minimum_eigenvalue(hamiltonian)\n",
    "\n",
    "# Create state vector from the ansatz using optimized parameters\n",
    "optimal_state = Statevector(ansatz.assign_parameters(result_vqe.optimal_parameters))\n",
    "\n",
    "# Print out the combination of qubits with the lowest energy\n",
    "print(f'Minimum eigenvalue (VQE): {result_vqe.eigenvalue.real}')\n",
    "optimal_segments_vqe = OptimizationApplication.sample_most_likely(optimal_state)\n",
    "print(f'Optimal segments (VQE): {optimal_segments_vqe}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visual Inspection\n",
    "\n",
    "Even if the tracking works successfully, expressing the answer as a string of 0s and 1s is a bit dry. Run the following code to visually confirm if correct tracks are found.\n",
    "\n",
    "This code viualizes the detector hits used in QUBO by projecting them onto a plane perpendicular to the beam axis and shows which detector hits are selected after optimization. The green lines correspond to found tracks and the blue lines, altogether with the green ones, correspond to all track candidates. In this exercise, only small number of qubits are used and therefore most of tracks are not found. However, it shows that a correct track is successfully found from the presence of green line."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from hepqpr.qallse import DataWrapper, Qallse, TrackRecreaterD\n",
    "from hepqpr.qallse.plotting import iplot_results, iplot_results_tracks\n",
    "from hepqpr.qallse.utils import diff_rows\n",
    "\n",
    "#optimal_segments = optimal_segments_vqe\n",
    "optimal_segments = optimal_segments_diag\n",
    "\n",
    "# Since each segment has ID, the data is passed to Qallse in the form of {ID: 0 or 1}\n",
    "#with h5py.File('data/QUBO_05pct_input.h5', 'r') as source:\n",
    "#   triplet_keys = map(lambda key: key.decode('UTF-8'), source['triplet_keys'][()])\n",
    "#\n",
    "#samples = dict(zip(triplet_keys, optimal_segments))\n",
    "samples = dict(zip(key_i, optimal_segments))\n",
    "\n",
    "# get the results\n",
    "all_doublets = Qallse.process_sample(samples)\n",
    "\n",
    "final_tracks, final_doublets = TrackRecreaterD().process_results(all_doublets)\n",
    "\n",
    "#dw = DataWrapper.from_path('data/event000001000-hits.csv')\n",
    "input_path = os.getcwd()+'/ds/'+prefix+'/event000001000-hits.csv'\n",
    "dw = DataWrapper.from_path(input_path)\n",
    "\n",
    "p, r, ms = dw.compute_score(final_doublets)\n",
    "trackml_score = dw.compute_trackml_score(final_tracks)\n",
    "\n",
    "print(f'SCORE  -- precision (%): {p * 100}, recall (%): {r * 100}, missing: {len(ms)}')\n",
    "print(f'          tracks found: {len(final_tracks)}, trackml score (%): {trackml_score * 100}')\n",
    "\n",
    "dims = ['x', 'y']\n",
    "_, missings, _ = diff_rows(final_doublets, dw.get_real_doublets())\n",
    "dout = 'plot-ising_found_tracks.html'\n",
    "iplot_results(dw, final_doublets, missings, dims=dims, filename=dout)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}